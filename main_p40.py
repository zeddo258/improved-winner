import pyaudio
import requests
import tkinter.font as tkFont
import ollama
import usb.core
import usb.util
from MeloTTS.melo.api import TTS
import pygame
from usb_4_mic_array.tuning import Tuning
import tkinter as tk
import numpy as np
import threading
import os
import json
import cv2


dev = usb.core.find(idVendor=0x2886, idProduct=0x0018)
Mic_tuning = ''
if dev:
    Mic_tuning = Tuning(dev)

# API endpoint URL
URL = "http://localhost:8080/upload-audio/"
URL_OLLAMA = "http://localhost:11435/api/chat"
RESPEAKER_RATE = 16000
RESPEAKER_CHANNELS = 1
RESPEAKER_WIDTH = 2
RESPEAKER_INDEX = 1 
CHUNK = 1024
RECORD_SECONDS = 2
MAX_SILENCE_FRAMES = 30


p = pyaudio.PyAudio()

  
def bytes_to_text(audio_bytes: bytes):
    files = {'file': ('microphone_audio.wav', audio_bytes, 'audio/wav')}
    response = requests.post(URL, files=files)
    if response.status_code == 200:
        result = response.json().get("result", "")
        print(result)
        return result
    else:
        print(f"Error: {response.status_code}")
        return ""

convo = []
def stream_response(prompt):
    convo.append({'role': 'user', 'content':prompt})
    response = ''
    stream = ollama.chat(model='llama3.1:70b', messages=convo, stream=True)
    print('\nASSISSTANT:')

    for chunk in stream:
        content = chunk['message']['content']
        response += content
        print(content, end='', flush=True)

    print('\n')
    convo.append({'role':'assisstant', 'content':response})


def start_listening(stream):
    print("Listening...")

    frames = []
    silence_frames = 0

    while silence_frames < MAX_SILENCE_FRAMES:
        data = stream.read(CHUNK)
        frames.append(data)

        if Mic_tuning.is_voice():  
            silence_frames = 0 
        else:
            silence_frames += 1  

    audio_bytes = b''.join(frames)
        
    # Convert audio to text using the server
    transcribed_text = bytes_to_text(audio_bytes)
    return transcribed_text


class ChatbotApp:
    def __init__(self, root):
        self.root = root
        self.root.title("Llama 3.1")

        self.root.geometry("1200x600")  # Adjusted size for webcam feed and chatbot UI
        font_style = tkFont.Font(family="Microsoft YaHei", size=16)
 
        # å¢žåŠ å°è©±æ¡†çš„å¤§å°ä¸¦è¨­ç½®å­—é«”
        self.text_area = tk.Text(root, height=30, width=80, font=font_style)  # è¨­ç½®å­—é«”å¤§å°
        self.text_area.pack(expand=True, fill=tk.BOTH)  # ä½¿ç”¨expandå’Œfillä½¿å°è©±æ¡†å¡«å……æ•´å€‹çª—å£

        # éº¥å…‹é¢¨æŒ‰éˆ•

        self.mic_button = tk.Button(root, text="ðŸŽ¤ é»žæ“Šèªªè©±", font=font_style, width=20, height=2, command=self.on_mic_click)  # è¨­ç½®æŒ‰éˆ•å¤§å°
        self.mic_button.pack(pady=20)  # ä½¿ç”¨ pady å¢žåŠ ä¸Šä¸‹é–“è·
        self.counter = 0
        # å„²å­˜å°è©±ç´€éŒ„
        system_prompt = "ä½ æ‰€åœ¨åœ°å€æ˜¯å°ç£,ä½ ä½¿ç”¨ç¹é«”ä¸­æ–‡å°è©±."
        self.convo = [{"role": "system", "content":system_prompt}]

        # Webcam Frame
        self.webcam_frame = tk.Label(root)
        self.webcam_frame.pack(side=tk.RIGHT, expand=True, fill=tk.BOTH)

        # Start the webcam thread
        self.running = True
        self.webcam_thread = threading.Thread(target=self.show_webcam_feed)
        self.webcam_thread.start()

    def show_webcam_feed(self):
        cap = cv2.VideoCapture(0)
        while self.running:
            ret, frame = cap.read()
            if ret:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                img = cv2.resize(frame, (400, 300))  # Resize for UI
                img = tk.PhotoImage(data=cv2.imencode('.png', img)[1].tobytes())
                self.webcam_frame.configure(image=img)
                self.webcam_frame.image = img
            else:
                break
        cap.release()
       
    def on_mic_click(self):
        # æŒ‰ä¸‹æŒ‰éˆ•æ™‚ï¼Œå•Ÿå‹•éŒ„éŸ³ä¸¦é€²è¡Œè™•ç†
        threading.Thread(target=self.listen_and_process).start()

    def listen_and_process(self):
        if ( self.counter == 0 ):
            first_prompt = f"""ä½ æ˜¯â€œspeakerâ€ï¼Œæˆ‘æ˜¯â€œlistenerâ€ã€‚
                            ç¾åœ¨â€œspeakerâ€æ˜¯(æœ‹å‹)ã€‚
                            æä¾›å¹«åŠ©å’Œè†è½å°æ–¹ç…©æƒ±å’Œçµ¦å°æ–¹å»ºè­°ã€‚
                            ç†è§£â€œlistenerâ€çš„ç”Ÿæ´»å¤§å°äº‹ã€æœªä¾†è¦åŠƒã€èª²æ¥­å•é¡Œã€æ„Ÿæƒ…å•é¡Œã€æ—…éŠç¶“é©—ã€ç¾Žé£Ÿã€ç¶“é©—åˆ†äº«ã€å„ç¨®æ„Ÿæƒ³ã€æœ‰è¶£çš„æ•…äº‹ã€å¹»æƒ³ã€‚
                            å›žæ‡‰â€œlistenerâ€æå‡ºçš„å•é¡Œï¼Œç°¡å–®èªªæ˜Žâ€œlistenerâ€çš„æ‰€æå‡ºçš„å•é¡Œã€‚
                            è‹¥ç„¡æ³•ç°¡å–®ä»¥å®¹æ˜“ç†è§£çš„å°è©±ï¼Œè§£ç­”â€œlistenerâ€çš„å•é¡Œï¼Œå°±è«‹â€œlistenerâ€å†æ›´å…·é«”èªªæ˜Žã€‚
                            è«‹ç”¨æ›´å£èªžåŒ–çš„èªžè¨€ï¼Œèˆ‡â€œlistenerâ€å°è©±ã€‚
                            ç†è§£å°è©±å…§å®¹ä»¥â€œspeakerâ€çš„è§’è‰²æ›ä½åŒç†â€œlistenerâ€ã€‚
                            â€œspeakerâ€çš„å›žæ‡‰éœ€è¦æŽ’é™¤éŽåº¦ç©æ¥µåŠéŽåº¦æ¨‚è§€ç”¨è©žã€‚
                            ä¾(æœ‹å‹)çš„è§’åº¦ï¼Œä½¿ç”¨500å­—ä»¥å…§çš„å›žæ‡‰ï¼Œä»¥æ”¯æŒåž‹å›žæ‡‰æ–¹å¼å›žè¦†â€œlistenerâ€ã€‚"""
            self.convo.append({'role': 'user', 'content': first_prompt})

        # é–‹å§‹éŒ„éŸ³ï¼Œä¸¦è™•ç†è½‰æ›å’Œå›žæ‡‰
        stream = p.open(
            rate=RESPEAKER_RATE,
            format=p.get_format_from_width(RESPEAKER_WIDTH),
            channels=RESPEAKER_CHANNELS,
            input=True,
            input_device_index=RESPEAKER_INDEX,
        )
        
        # å‘¼å«éŒ„éŸ³åŠè™•ç†å‡½æ•¸
        self.update_conversation("Listening...\n")  # UIæ›´æ–°ç‚ºæ­£åœ¨è½
        transcribed_text = start_listening(stream)  # è¿”å›žè½‰éŒ„æ–‡æœ¬
        stream.stop_stream()
        stream.close()

        if "None" not in transcribed_text:
            # æ›´æ–°ç”¨æˆ¶æå•
            self.update_conversation(f"ä½ : {transcribed_text}\n")
            # å‘¼å«chatbotæ¨¡åž‹è™•ç†å›žæ‡‰
            self.process_response(transcribed_text)
    
    def update_conversation(self, message):
        self.text_area.insert(tk.END, f"{message}")
        self.text_area.see(tk.END)  # è‡ªå‹•æ»¾å‹•åˆ°æœ€æ–°å°è©±

    def process_response(self, prompt):
        self.convo.append({'role': 'user', 'content': prompt})
        self.counter += 1
        response = ''
        stream = requests.post(
            url=URL_OLLAMA,
            json= {
                "model":'llama3.1:70b', 
                "messages":self.convo, 
                "stream":True,
            }
        )
        
        self.update_conversation("æ©Ÿå™¨äºº: ") 
        for chunk in stream.iter_lines():
            body = json.loads(chunk)
            if "error" in body:
                raise Exception(body["error"])
            if body.get("done") is False:
                message = body.get("message", "")
                content = message.get("content", "")
                response += content
            self.update_conversation(content)

        self.update_conversation("\n")
        self.text_to_speech(response)
        self.convo.append({'role': 'assistant', 'content': response})

    def text_to_speech(self, text):

        # Speed is adjustable
        speed = 0.8
        device = 'cuda:0'

        model = TTS(language='ZH', device=device)
        speaker_ids = model.hps.data.spk2id

        output_path = 'zh.wav'
        model.tts_to_file(text, speaker_ids['ZH'], output_path, speed=speed)

        # ä½¿ç”¨ pygame æ’­æ”¾éŸ³é »
        pygame.mixer.init()
        pygame.mixer.music.load(output_path)
        pygame.mixer.music.play()

        # ç­‰å¾…éŸ³é »æ’­æ”¾å®Œç•¢
        while pygame.mixer.music.get_busy():
            continue
        # åœæ­¢ä¸¦é€€å‡º pygame.mixer

        pygame.mixer.music.stop()
        pygame.mixer.quit()
        # åˆªé™¤éŸ³é »æ–‡ä»¶
        os.remove(output_path)

if __name__ == "__main__":
    root = tk.Tk()
    app = ChatbotApp(root)
    root.mainloop()
