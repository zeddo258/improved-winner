import pyaudio
import requests
import tkinter.font as tkFont
import ollama
import usb.core
import usb.util
from MeloTTS.melo.api import TTS
from opencc import OpenCC
import pygame
from usb_4_mic_array.tuning import Tuning
from collections import Counter
import tkinter as tk
import numpy as np
import threading
import os
import cv2 
from threading import Lock
import io
import re 
from PIL import Image, ImageTk 


dev = usb.core.find(idVendor=0x2886, idProduct=0x0018)
Mic_tuning = ''
if dev:
    Mic_tuning = Tuning(dev)
converter = OpenCC('s2t')  # æˆ–ä½¿ç”¨ 's2tw' è½‰ç‚ºå°ç£ç¹é«”ä¸­æ–‡
# API endpoint URL
SENSEVOICE_URL = "http://localhost:8080/upload-audio/"
POSTER_URL = "http://localhost:8000/ask/"

RESPEAKER_RATE = 16000
RESPEAKER_CHANNELS = 1
RESPEAKER_WIDTH = 2
RESPEAKER_INDEX = 1 
CHUNK = 1024
RECORD_SECONDS = 2
MAX_SILENCE_FRAMES = 30


p = pyaudio.PyAudio()

  
def bytes_to_text(audio_bytes: bytes):
    files = {'file': ('microphone_audio.wav', audio_bytes, 'audio/wav')}
    response = requests.post(SENSEVOICE_URL, files=files)
    if response.status_code == 200:
        result = response.json().get("result", "")
        print(result)
        return result
    else:
        print(f"Error SenseVoice: {response.status_code}")
        return ""

convo = []
def stream_response(prompt):
    convo.append({'role': 'user', 'content':prompt})
    response = ''
    stream = ollama.chat(model='llama3.1', messages=convo, stream=True)
    print('\nASSISSTANT:')

    for chunk in stream:
        content = chunk['message']['content']
        response += content
        print(content, end='', flush=True)

    print('\n')
    convo.append({'role':'assisstant', 'content':response})



class ChatbotApp:
    def __init__(self, root):
        self.root = root
        self.root.title("Llama 3.1")
        self.root.geometry("1200x600")  # Adjusted size for side-by-side layout

        self.poster_result = []  # Shared variable for the latest frame
        self.poster_result_lock = Lock()  # Lock to ensure thread-safe access to the frame

        font_style = tkFont.Font(family="Microsoft YaHei", size=16)

        # Configure grid for the root
        self.root.grid_rowconfigure(0, weight=1)
        self.root.grid_columnconfigure(0, weight=1)
        self.root.grid_columnconfigure(1, weight=1)

        # Chatbot Frame (Left Section)
        self.chatbot_frame = tk.Frame(self.root, bg="white")
        self.chatbot_frame.grid(row=0, column=0, sticky="nsew", padx=10, pady=10)

        # Configure rows in the chatbot frame
        self.chatbot_frame.grid_rowconfigure(0, weight=1)  # Text area row
        self.chatbot_frame.grid_rowconfigure(1, weight=0)  # Button row
        self.chatbot_frame.grid_columnconfigure(0, weight=1)

        # Text Area for Chatbot
        self.text_area = tk.Text(self.chatbot_frame, font=font_style, wrap=tk.WORD)
        self.text_area.grid(row=0, column=0, sticky="nsew", padx=5, pady=5)

        # Microphone Button
        self.mic_button = tk.Button(self.chatbot_frame, text="ğŸ¤ Click to Speak", font=font_style, command=self.on_mic_click)
        self.mic_button.grid(row=1, column=0, pady=5, sticky="ew")

        # Webcam Frame (Right Section)
        self.webcam_frame = tk.Frame(self.root, bg="white")
        self.webcam_frame.grid(row=0, column=1, sticky="nsew", padx=10, pady=10)

        # Webcam Label for Displaying Feed
        self.webcam_label = tk.Label(self.webcam_frame, bg="white")
        self.webcam_label.pack(expand=True, fill=tk.BOTH)

        self.face_emotion_label = tk.Label(
            self.webcam_frame, text="Facial emotion: ", font=font_style, bg="white", anchor="w", relief="sunken"
        )
        self.face_emotion_label.pack(fill=tk.X, padx=5, pady=5)

        self.voice_emotion_label = tk.Label(
            self.webcam_frame, text="Voice emotion: ", font=font_style, bg="white", anchor="w", relief="sunken"
        )
        self.voice_emotion_label.pack(fill=tk.X, padx=5, pady=5)

        self.voice_emotion_label = tk.Label(
            self.webcam_frame, text="Text emotion: ", font=font_style, bg="white", anchor="w", relief="sunken"
        )
        self.voice_emotion_label.pack(fill=tk.X, padx=5, pady=5)


        # å„²å­˜å°è©±ç´€éŒ„
        system_prompt = "ä½ æ‰€åœ¨åœ°å€æ˜¯å°ç£,ä½ ä½¿ç”¨ç¹é«”ä¸­æ–‡å°è©±."
        self.convo = [{"role": "system", "content":system_prompt}]


        # Start the webcam thread
        self.running = True
        self.is_recording = False
        self.webcam_thread = threading.Thread(target=self.show_webcam_feed)
        self.webcam_thread.start()
        self.counter = 0

    def show_webcam_feed(self):
        """Continuously display webcam feed and send frames to API during recording."""
        cap = cv2.VideoCapture(0)
        while self.running:
            ret, frame = cap.read()
            if ret:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frame = cv2.resize(frame, (400, 300))


                img = ImageTk.PhotoImage(Image.fromarray(frame))
                self.webcam_label.configure(image=img)
                self.webcam_label.image = img

                # If recording, process the frame
                if self.is_recording:
                    detected_emotion = self.send_frame_to_api(frame)
                    with self.poster_result_lock:
                        self.poster_result.append(detected_emotion)
            else:
                break

        cap.release()

    def send_frame_to_api(self, frame):
        # Save pil image to IO buffer
        pil_img = Image.fromarray(frame)
        buf = io.BytesIO()
        pil_img.save(buf, format='JPEG')
        buf.seek(0)

        files = {'image': ('image.jpg', buf, 'image/jpeg') }

        try: 
            r = requests.post(POSTER_URL, files=files)
            if r.status_code == 200:
                prediction = r.json().get("result")
                return prediction
        except Exception as e:
            print(f"Failed to send frame:{e}")

    def on_mic_click(self):
        """Toggle recording state and handle audio processing."""
        self.is_recording = not self.is_recording

        if self.is_recording:
            self.mic_button.config(text="ğŸ”´ Recording... Click to Stop")
            print("Recording started")
            threading.Thread(target=self.listen_and_process).start()
        else:
            self.mic_button.config(text="ğŸ¤ Click to Speak")
            print("Recording stopped")

    def start_listening(self, stream):
        print("Listening...")

        frames = []
        while self.is_recording:
            data = stream.read(CHUNK)
            frames.append(data)


        audio_bytes = b''.join(frames)
        # Convert audio to text using the server
        transcribed_text = bytes_to_text(audio_bytes)
        print(transcribed_text)
        return transcribed_text

    def listen_and_process(self):
        if ( self.counter == 0 ):
            first_prompt = f"""ä½ æ˜¯â€œspeakerâ€ï¼Œæˆ‘æ˜¯â€œlistenerâ€ã€‚
                            ç¾åœ¨â€œspeakerâ€æ˜¯(æœ‹å‹)ã€‚
                            æä¾›å¹«åŠ©å’Œè†è½å°æ–¹ç…©æƒ±å’Œçµ¦å°æ–¹å»ºè­°ã€‚
                            ç†è§£â€œlistenerâ€çš„ç”Ÿæ´»å¤§å°äº‹ã€æœªä¾†è¦åŠƒã€èª²æ¥­å•é¡Œã€æ„Ÿæƒ…å•é¡Œã€æ—…éŠç¶“é©—ã€ç¾é£Ÿã€ç¶“é©—åˆ†äº«ã€å„ç¨®æ„Ÿæƒ³ã€æœ‰è¶£çš„æ•…äº‹ã€å¹»æƒ³ã€‚
                            å›æ‡‰â€œlistenerâ€æå‡ºçš„å•é¡Œï¼Œç°¡å–®èªªæ˜â€œlistenerâ€çš„æ‰€æå‡ºçš„å•é¡Œã€‚
                            è‹¥ç„¡æ³•ç°¡å–®ä»¥å®¹æ˜“ç†è§£çš„å°è©±ï¼Œè§£ç­”â€œlistenerâ€çš„å•é¡Œï¼Œå°±è«‹â€œlistenerâ€å†æ›´å…·é«”èªªæ˜ã€‚
                            è«‹ç”¨æ›´å£èªåŒ–çš„èªè¨€ï¼Œèˆ‡â€œlistenerâ€å°è©±ã€‚
                            ç†è§£å°è©±å…§å®¹ä»¥â€œspeakerâ€çš„è§’è‰²æ›ä½åŒç†â€œlistenerâ€ã€‚
                            â€œspeakerâ€çš„å›æ‡‰éœ€è¦æ’é™¤éåº¦ç©æ¥µåŠéåº¦æ¨‚è§€ç”¨è©ã€‚
                            ä¾(æœ‹å‹)çš„è§’åº¦ï¼Œä½¿ç”¨500å­—ä»¥å…§çš„å›æ‡‰ï¼Œä»¥æ”¯æŒå‹å›æ‡‰æ–¹å¼å›è¦†â€œlistenerâ€ã€‚"""
            self.convo.append({'role': 'user', 'content': first_prompt})

        # é–‹å§‹éŒ„éŸ³ï¼Œä¸¦è™•ç†è½‰æ›å’Œå›æ‡‰
        stream = p.open(
            rate=RESPEAKER_RATE,
            format=p.get_format_from_width(RESPEAKER_WIDTH),
            channels=RESPEAKER_CHANNELS,
            input=True,
            input_device_index=RESPEAKER_INDEX,
        )
        
        # å‘¼å«éŒ„éŸ³åŠè™•ç†å‡½æ•¸
        self.update_conversation("Listening...\n")  # UIæ›´æ–°ç‚ºæ­£åœ¨è½
        transcribed_text = self.start_listening(stream)  # è¿”å›è½‰éŒ„æ–‡æœ¬
        emotion = ""
        text = ""
        for item in transcribed_text:
            text_content = item['text']  # Extract the string from the dictionary
            # Extract emotion
            emotion_match = re.search(r'<\|([A-Z]+)\|>', text_content)
            emotion = emotion_match.group(1) if emotion_match else "Unknown"

            # Extract text content
            text_match = re.search(r'<\|withitn\|>(.+)', text_content)
            text = text_match.group(1) if text_match else ""

        text = converter.convert(text)
        stream.stop_stream()
        stream.close()

        if "None" not in text:
            with self.poster_result_lock:
                if self.poster_result:
                    most_common_emotion = Counter(self.poster_result).most_common(1)[0][0]
                else:
                    most_common_emotion = "Neutral"
            # æ›´æ–°ç”¨æˆ¶æå•
            self.update_conversation(f"ä½ : {text}\n")
            if emotion == "Unknown":
                self.voice_emotion_label.config(text=f"Voice Emotion: Neutral")
            else:
                self.voice_emotion_label.config(text=f"Voice Emotion: {emotion}")

            # å‘¼å«chatbotæ¨¡å‹è™•ç†å›æ‡‰
            self.process_response(text, poster_emotion=most_common_emotion, sense_voice_emotion=emotion)
    
    def update_conversation(self, message):
        self.text_area.insert(tk.END, f"{message}")
        self.text_area.see(tk.END)  # è‡ªå‹•æ»¾å‹•åˆ°æœ€æ–°å°è©±

    def process_response(self, prompt, poster_emotion, sense_voice_emotion):
        prompt = f"Listenerè‡‰éƒ¨è¡¨æƒ…ç‚º : {poster_emotion} ä»–çš„è²èª¿è¡¨æƒ…ç‚º :{sense_voice_emotion} Listenerè·Ÿä½ èªª :{prompt}"
        self.convo.append({'role': 'user', 'content': prompt})
        self.counter += 1
        response = ''
        self.face_emotion_label.config(text=f"Facial Emotion: {poster_emotion}")
        stream = ollama.chat(model='llama3.1', messages=self.convo, stream=True)
        
        self.update_conversation("æ©Ÿå™¨äºº: ") 
        for chunk in stream:
            content = chunk['message']['content']
            response += content
            self.update_conversation(content)
        self.update_conversation("\n")
        self.text_to_speech(response)
        self.convo.append({'role': 'assistant', 'content': response})

    def text_to_speech(self, text):

        # Speed is adjustable
        speed = 1
        device = 'cuda:0'

        model = TTS(language='ZH', device=device)
        speaker_ids = model.hps.data.spk2id

        output_path = 'zh.wav'
        model.tts_to_file(text, speaker_ids['ZH'], output_path, speed=speed)

        # ä½¿ç”¨ pygame æ’­æ”¾éŸ³é »
        pygame.mixer.init()
        pygame.mixer.music.load(output_path)
        pygame.mixer.music.play()

        # ç­‰å¾…éŸ³é »æ’­æ”¾å®Œç•¢
        while pygame.mixer.music.get_busy():
            continue
        # åœæ­¢ä¸¦é€€å‡º pygame.mixer

        pygame.mixer.music.stop()
        pygame.mixer.quit()
        # åˆªé™¤éŸ³é »æ–‡ä»¶
        os.remove(output_path)

if __name__ == "__main__":
    root = tk.Tk()
    app = ChatbotApp(root)
    root.mainloop()
